[[reports]]
level = 'Warning'
analyzer = 'UnsafeDestructor'
description = 'unsafe block detected in drop'
location = 'src/lib.rs:429:1: 471:2'
source = '''
impl<T: Eq + Hash + Send> Drop for ArcIntern<T> {
    fn drop(&mut self) {
        // (Quoting from std::sync::Arc again): Because `fetch_sub` is
        // already atomic, we do not need to synchronize with other
        // threads unless we are going to delete the object. This same
        // logic applies to the below `fetch_sub` to the `weak` count.
        let count_was = unsafe { (*self.pointer).count.fetch_sub(1, Ordering::Release) };
        if count_was == 2 {
            // Looks like we are ready to remove from the HashSet.  Ourselves and the
            // HashSet hold the only two pointers to this datum.
            let _removed: Arc<T>; // Free the pointer *after* releasing the Mutex.
            let mut m = Self::get_mutex().lock().unwrap();
            // We shouldn't need to be more stringent than relaxed ordering below,
            // since we are holding a Mutex.
            let count_is = unsafe { (*self.pointer).count.load(Ordering::Relaxed) };
            if count_is == 1 {
                // Here we check the count again in case the pointer was added while
                // we waited for the Mutex.  The count cannot *increase* in a racy way
                // because that would require either that there be another copy of the
                // ArcIntern (which we know doesn't exist because of the count) or 
                // someone holding the Mutex.
                if let Some(interned) = m.take(&self.clone_to_arc()) {
                    assert_eq!(self.pointer, interned.pointer);
                    _removed = interned;
                    // if interned.pointer != self.pointer {
                    //     m.insert(interned);
                    // }
                }
            }
        } else if count_was == 1 {
            // (Quoting from std::sync::Arc again): This fence is
            // needed to prevent reordering of use of the data and
            // deletion of the data.  Because it is marked `Release`,
            // the decreasing of the reference count synchronizes with
            // this `Acquire` fence. This means that use of the data
            // happens before decreasing the reference count, which
            // happens before this fence, which happens before the
            // deletion of the data.
            std::sync::atomic::fence(Ordering::Acquire);
            let _removed = unsafe { Box::from_raw(self.pointer as *mut RefCount<T>) };
        }
    }
}'''

[[reports]]
level = 'Warning'
analyzer = 'UnsafeDestructor'
description = 'unsafe block detected in drop'
location = 'src/lib.rs:473:1: 493:2'
source = '''
impl<T: Eq + Hash + Send> Drop for Arc<T> {
    fn drop(&mut self) {
        // (Quoting from std::sync::Arc again): Because `fetch_sub` is
        // already atomic, we do not need to synchronize with other
        // threads unless we are going to delete the object. This same
        // logic applies to the below `fetch_sub` to the `weak` count.
        let count_was = unsafe { (*self.pointer).count.fetch_sub(1, Ordering::Release) };
        if count_was == 1 {
            // (Quoting from std::sync::Arc again): This fence is
            // needed to prevent reordering of use of the data and
            // deletion of the data.  Because it is marked `Release`,
            // the decreasing of the reference count synchronizes with
            // this `Acquire` fence. This means that use of the data
            // happens before decreasing the reference count, which
            // happens before this fence, which happens before the
            // deletion of the data.
            std::sync::atomic::fence(Ordering::Acquire);
            let _removed = unsafe { Box::from_raw(self.pointer as *mut RefCount<T>) };
        }
    }
}'''
